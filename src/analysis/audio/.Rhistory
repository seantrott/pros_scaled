summary(model_slope)
model_change = lmer(data = filter(df_audio, form == "non-conventional"),
overall_change ~ intent2 +
(1 +intent2 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
model_null = lmer(data = filter(df_audio, form == "non-conventional"),
overall_change ~
(1 +intent2 | topic) +
(1 +intent2 | speaker),
REML = FALSE)
anova(model_change, model_null)
summary(model_change)
model_slope = lmer(data = filter(df_audio, form == "non-conventional"),
f0_slope ~ intent2 +
(1 +intent2 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
model_null = lmer(data = filter(df_audio, form == "non-conventional"),
f0_slope ~
(1 +intent2 | topic) +
(1 +intent2 | speaker),
REML = FALSE)
anova(model_slope, model_null)
summary(model_slope)
df_audio %>%
ggplot(aes(x = form,
y = overall_change,
color = label)) +
stat_summary (fun = function(x){mean(x)},
fun.min = function(x){mean(x) - 2*sd(x)/sqrt(length(x))},
fun.max = function(x){mean(x) + 2*sd(x)/sqrt(length(x))},
geom= 'pointrange',
position=position_dodge(width=0.95)) +
geom_hline(yintercept = 0, linetype = "dotted") +
labs(x = "Form",
y = "Change in F0 (final - initial)",
color = "Intent") +
theme_minimal()
ggsave("../../../Figures/supplementary_overall_change.png", dpi = 300)
df_audio$form = factor(df_audio$form, levels = c("non-conventional", "conventional"))
model_change = lmer(data = df_audio,
overall_change ~ intent2 * form +
(1 + intent2 | topic) +
(1 + intent2 | speaker),
control=lmerControl(optimizer="bobyqa"),
REML = FALSE)
df_audio$form = factor(df_audio$form, levels = c("non-conventional", "conventional"))
model_change = lmer(data = df_audio,
overall_change ~ intent2 * form +
(1 | topic) +
(1 + intent2 | speaker),
control=lmerControl(optimizer="bobyqa"),
REML = FALSE)
df_audio$form = factor(df_audio$form, levels = c("non-conventional", "conventional"))
model_change = lmer(data = df_audio,
overall_change ~ intent2 * form +
(1 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
model_reduced = lmer(data = df_audio,
overall_change ~ intent2 + form +
(1 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
anova(model_change, model_reduced)
model_slope = lmer(data = df_audio,
f0_slope ~ intent2 * form +
(1 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
model_reduced = lmer(data = df_audio,
f0_slope ~ intent2 + form +
(1 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
anova(model_slope, model_reduced)
summary(model_slope)
table(df_audio$form)
df_audio %>%
ggplot(aes(x = form,
y = overall_change,
color = label)) +
stat_summary (fun = function(x){mean(x)},
fun.min = function(x){mean(x) - 2*sd(x)/sqrt(length(x))},
fun.max = function(x){mean(x) + 2*sd(x)/sqrt(length(x))},
geom= 'pointrange',
position=position_dodge(width=0.95)) +
geom_hline(yintercept = 0, linetype = "dotted") +
labs(x = "Form",
y = "Degree of Rise (final - initial)",
color = "Intent") +
theme_minimal()
ggsave("../../../Figures/supplementary_overall_change.png", dpi = 300)
setwd("/Users/seantrott/Dropbox/UCSD/Research/IndirectSpeechActs/Prosody/pros_scaled/src/analysis/audio")
df_audio = read_csv("../../../data/processed/audio/audio_features_final_segment.csv")
df_audio$label = factor(df_audio$intent2)
df_audio$form = factor(df_audio$form)
nrow(df_audio)
table(df_audio$speaker, df_audio$label)
## How many duplicates
df_audio %>%
group_by(hash) %>%
filter(n()>1)
## Get distinct
df_audio = df_audio %>%
distinct(hash, .keep_all = TRUE)
nrow(df_audio)
df_audio %>%
ggplot(aes(x = form,
y = overall_change,
color = label)) +
stat_summary (fun = function(x){mean(x)},
fun.min = function(x){mean(x) - 2*sd(x)/sqrt(length(x))},
fun.max = function(x){mean(x) + 2*sd(x)/sqrt(length(x))},
geom= 'pointrange',
position=position_dodge(width=0.95)) +
geom_hline(yintercept = 0, linetype = "dotted") +
labs(x = "Form",
y = "Degree of Rise (final - initial)",
color = "Intent") +
theme_minimal()
df_audio$form = factor(df_audio$form, levels = c("non-conventional", "conventional"))
model_change = lmer(data = df_audio,
overall_change ~ intent2 * form +
(1 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
model_reduced = lmer(data = df_audio,
overall_change ~ intent2 + form +
(1 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
anova(model_change, model_reduced)
summary(model_change)
model_slope = lmer(data = df_audio,
f0_slope ~ intent2 * form +
(1 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
model_reduced = lmer(data = df_audio,
f0_slope ~ intent2 + form +
(1 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
anova(model_slope, model_reduced)
summary(model_slope)
ggplot(aes(x = form,
y = overall_change,
color = label)) +
stat_summary (fun = function(x){mean(x)},
fun.min = function(x){mean(x) - 2*sd(x)/sqrt(length(x))},
fun.max = function(x){mean(x) + 2*sd(x)/sqrt(length(x))},
geom= 'pointrange',
position=position_dodge(width=0.95)) +
geom_hline(yintercept = 0, linetype = "dotted") +
labs(x = "Form",
y = "Degree of Rise (final - initial)",
color = "Intent") +
theme_minimal()
df_audio %>%
ggplot(aes(x = form,
y = overall_change,
color = label)) +
stat_summary (fun = function(x){mean(x)},
fun.min = function(x){mean(x) - 2*sd(x)/sqrt(length(x))},
fun.max = function(x){mean(x) + 2*sd(x)/sqrt(length(x))},
geom= 'pointrange',
position=position_dodge(width=0.95)) +
geom_hline(yintercept = 0, linetype = "dotted") +
labs(x = "Form",
y = "Degree of Rise (final - initial)",
color = "Intent") +
theme_minimal()
ggsave("../../../Figures/supplementary_overall_change.png", dpi = 300)
library(tidyverse)
library(forcats)
library(lme4)
library(broom)
setwd("/Users/seantrott/Dropbox/UCSD/Research/IndirectSpeechActs/Prosody/pros_scaled/src/analysis/audio")
df_audio = read_csv("../../../data/processed/audio/audio_features_scaled_stimuli.csv")
df_audio$label = factor(df_audio$label)
df_audio$form = factor(df_audio$form)
nrow(df_audio)
table(df_audio$speaker, df_audio$label)
length(unique(df_audio$ppt_id))
```
## Remove subjects with less than 24 observations
We also exclude participant 16, who was recorded with a lower-quality laptop microphone. (Note that the exclusion does not really affect the primary outcomes reported.)
```{r}
n_obs = df_audio %>%
group_by(ppt_id) %>%
summarise(count = n())
hist(n_obs$count)
complete = n_obs %>%
filter(count == 24)
## print ppt ids with < 24 observations
n_obs %>%
filter(count < 24)
## Select only ppts with 24 observations
df_audio_filtered = df_audio %>%
filter(ppt_id %in% complete$ppt_id)
nrow(df_audio_filtered)
## How many remaining?
length(unique(df_audio_filtered$ppt_id))
## Also remove #16,
df_audio_filtered = df_audio_filtered %>%
filter(ppt_id != 16)
length(unique(df_audio_filtered$ppt_id))
unique(df_audio_filtered$ppt_id)
nrow(df_audio_filtered)
```
## Descriptive statistics
```{r}
by_intent = df_audio_filtered %>%
group_by(label) %>%
summarise(mean_f0 = mean(mean_f0),
range_f0 = mean(range_f0),
sd_f0 = mean(sd_f0),
duration_f0 = mean(duration_f0),
slope_f0 = mean(slope_f0),
mean_intensity = mean(mean_intensity),
sd_intensity = mean(sd_intensity))
by_intent
```
## Normalize variables by speaker
Now we z-score our variables, by speaker. E.g., for each speaker, we normalize each observation by z-scoring it *to that speaker*.
```{r}
df_audio_filtered = df_audio_filtered %>%
group_by(speaker) %>%
mutate(mean_f0_z_score = scale(mean_f0),
duration_f0_z_score = scale(duration_f0),
range_f0_z_score = scale(range_f0),
sd_f0_z_score = scale(sd_f0),
slope_f0_z_score = scale(slope_f0),
mean_intensity_z_score = scale(mean_intensity),
sd_intensity_z_score = scale(sd_intensity)
)
```
# Machine learning classifier
## Do acoustic features help us classify intent overall?
For a first-pass, we recode "statements" and "questions" as non-requests.
```{r}
df_audio_filtered$label_recoded = fct_recode(df_audio_filtered$label,
"non-request" = "statement",
"non-request" = "question")
```
### Logistic regression
```{r}
set.seed(123)
df_audio_filtered$index = c(1: nrow(df_audio_filtered))
prob_outputs = c()
for (iterating_index in c(1:nrow(df_audio_filtered))) {
train = df_audio_filtered %>%
filter(index != iterating_index)
test = df_audio_filtered %>%
filter(index == iterating_index)
model_core = glm(data = train,
label_recoded ~
mean_f0_z_score * form +
range_f0_z_score * form +
sd_f0_z_score * form +
duration_f0_z_score * form +
slope_f0_z_score * form +
mean_intensity_z_score * form +
sd_intensity_z_score * form,
family = binomial())
probabilities = predict(model_core, test, type="response")
prob_outputs[iterating_index] = probabilities
}
df_audio_filtered$lr_probs = prob_outputs
df_audio_filtered %>%
ggplot(aes(x = lr_probs,
fill = label_recoded)) +
geom_density(alpha = .6) +
labs(x = "P(Request)",
title = "Logistic regression predictions by speaker intent",
fill = "Intent") +
theme_minimal()
df_audio_filtered %>%
ggplot(aes(x = lr_probs,
fill = label_recoded)) +
geom_density(alpha = .6) +
labs(x = "P(Request)",
title = "Logistic regression predictions by speaker intent",
fill = "Intent") +
theme_minimal() +
theme(
axis.title.x = element_text(size = 16),
axis.text.x = element_text(size = 14),
axis.text.y = element_text(size = 10),
axis.title.y = element_text(size = 16),
strip.text.x = element_text(size = 16),
title = element_text(size = 16),
legend.text = element_text(size = 16),
legend.title = element_text(size = 16))
ggsave("../../../Figures/classifier.png", dpi = 300)
df_audio_filtered %>%
ggplot(aes(x = lr_probs,
fill = label_recoded)) +
geom_density(alpha = .6) +
labs(x = "P(Request)",
title = "Logistic regression predictions by speaker intent",
fill = "Intent") +
theme_minimal() +
theme(
axis.title.x = element_text(size = 16),
axis.text.x = element_text(size = 14),
axis.text.y = element_text(size = 10),
axis.title.y = element_text(size = 16),
strip.text.x = element_text(size = 16),
title = element_text(size = 16),
legend.text = element_text(size = 20),
legend.title = element_text(size = 16))
df_audio_filtered %>%
ggplot(aes(x = lr_probs,
fill = label_recoded)) +
geom_density(alpha = .6) +
labs(x = "P(Request)",
title = "Logistic regression predictions by speaker intent",
fill = "Intent") +
theme_minimal() +
theme(
axis.title.x = element_text(size = 16),
axis.text.x = element_text(size = 14),
axis.text.y = element_text(size = 10),
axis.title.y = element_text(size = 16),
strip.text.x = element_text(size = 16),
title = element_text(size = 16),
legend.text = element_text(size = 20),
legend.title = element_text(size = 20))
ggsave("../../../Figures/classifier.png", dpi = 300)
ggsave("../../../Figures/classifier.png", dpi = 300)
p.adjust(c(.001, .19), method = "holm")
setwd("/Users/seantrott/Dropbox/UCSD/Research/IndirectSpeechActs/Prosody/pros_scaled/src/analysis/audio")
df_audio = read_csv("../../../data/processed/audio/audio_features_final_segment.csv")
df_audio$label = factor(df_audio$intent2)
df_audio$form = factor(df_audio$form)
nrow(df_audio)
table(df_audio$speaker, df_audio$label)
## How many duplicates
df_audio %>%
group_by(hash) %>%
filter(n()>1)
## Get distinct
df_audio = df_audio %>%
distinct(hash, .keep_all = TRUE)
nrow(df_audio)
```
## Descriptive statistics
```{r}
by_intent = df_audio %>%
group_by(form, label) %>%
summarise(mean_change = mean(overall_change),
sd_change = sd(overall_change),
mean_slope = mean(f0_slope),
sd_slope = sd(f0_slope))
by_intent
```
## Normalize variables by speaker
Now we z-score our variables, by speaker. E.g., for each speaker, we normalize each observation by z-scoring it *to that speaker*.
```{r}
df_audio = df_audio %>%
group_by(speaker) %>%
mutate(overall_change_z = mean(overall_change),
f0_slope_z = mean(f0_slope)
)
set.seed(123)
df_audio$index = c(1: nrow(df_audio))
prob_outputs = c()
for (iterating_index in c(1:nrow(df_audio))) {
train = df_audio %>%
filter(index != iterating_index)
test = df_audio %>%
filter(index == iterating_index)
model_core = glm(data = train,
label ~
overall_change * form +
f0_slope * form,
family = binomial())
probabilities = predict(model_core, test, type="response")
prob_outputs[iterating_index] = probabilities
}
df_audio$lr_probs = prob_outputs
df_audio %>%
ggplot(aes(x = lr_probs,
fill = label)) +
geom_density(alpha = .6) +
labs(x = "P(Request)",
title = "Logistic regression predictions by speaker intent",
fill = "Intent") +
theme_minimal()
df_audio$lr_prediction = ifelse(df_audio$lr_probs > 0.5,
"request","non-request")
df_audio$lr_correct = df_audio$lr_prediction == df_audio$label
mean(df_audio$lr_correct)
df_audio$form = factor(df_audio$form, levels = c("non-conventional", "conventional"))
model_change = lmer(data = df_audio,
overall_change ~ intent2 * form +
(1 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
model_reduced = lmer(data = df_audio,
overall_change ~ intent2 + form +
(1 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
anova(model_change, model_reduced)
summary(model_change)
model_slope = lmer(data = df_audio,
f0_slope ~ intent2 * form +
(1 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
model_reduced = lmer(data = df_audio,
f0_slope ~ intent2 + form +
(1 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
anova(model_slope, model_reduced)
summary(model_slope)
k = anova(model_slope, model_reduced)
k$`Pr(>Chisq)`
k$`Pr(>Chisq)`[2]
p_change = anova(model_change, model_reduced)$`Pr(>Chisq)`[2]
p_slope = anova(model_slope, model_reduced)$`Pr(>Chisq)`[2]
p.adjust(c(p_change, p_slope), method = "holm")
anova(model_change, model_reduced)
df_audio$form = factor(df_audio$form, levels = c("non-conventional", "conventional"))
model_change = lmer(data = df_audio,
overall_change ~ intent2 * form +
(1 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
model_reduced = lmer(data = df_audio,
overall_change ~ intent2 + form +
(1 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
anova(model_change, model_reduced)
summary(model_change)
p_change = anova(model_change, model_reduced)$`Pr(>Chisq)`[2]
model_slope = lmer(data = df_audio,
f0_slope ~ intent2 * form +
(1 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
model_reduced = lmer(data = df_audio,
f0_slope ~ intent2 + form +
(1 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
anova(model_slope, model_reduced)
summary(model_slope)
p_slope = anova(model_slope, model_reduced)$`Pr(>Chisq)`[2]
p.adjust(c(p_change, p_slope), method = "holm")
model_change = lmer(data = filter(df_audio, form == "conventional"),
overall_change ~ intent2 +
(1 +intent2 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
model_null = lmer(data = filter(df_audio, form == "conventional"),
overall_change ~
(1 +intent2 | topic) +
(1 +intent2 | speaker),
REML = FALSE)
anova(model_change, model_null)
summary(model_change)
p_change = anova(model_change, model_null)$`Pr(>Chisq)`[2]
model_slope = lmer(data = filter(df_audio, form == "conventional"),
f0_slope ~ intent2 +
(1 +intent2 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
model_null = lmer(data = filter(df_audio, form == "conventional"),
f0_slope ~
(1 +intent2 | topic) +
(1 +intent2 | speaker),
REML = FALSE)
anova(model_slope, model_null)
summary(model_slope)
p_slope = anova(model_slope, model_null)$`Pr(>Chisq)`[2]
p.adjust(c(p_change, p_slope), method = "holm")
p_change
p.adjust(c(p_change, p_slope), method = "holm")
model_change = lmer(data = filter(df_audio, form == "non-conventional"),
overall_change ~ intent2 +
(1 +intent2 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
model_null = lmer(data = filter(df_audio, form == "non-conventional"),
overall_change ~
(1 +intent2 | topic) +
(1 +intent2 | speaker),
REML = FALSE)
anova(model_change, model_null)
summary(model_change)
p_change = anova(model_change, model_null)$`Pr(>Chisq)`[2]
model_slope = lmer(data = filter(df_audio, form == "non-conventional"),
f0_slope ~ intent2 +
(1 +intent2 | topic) +
(1 + intent2 | speaker),
REML = FALSE)
model_null = lmer(data = filter(df_audio, form == "non-conventional"),
f0_slope ~
(1 +intent2 | topic) +
(1 +intent2 | speaker),
REML = FALSE)
anova(model_slope, model_null)
summary(model_slope)
p_slope = anova(model_slope, model_null)$`Pr(>Chisq)`[2]
p.adjust(c(p_change, p_slope), method = "holm")
